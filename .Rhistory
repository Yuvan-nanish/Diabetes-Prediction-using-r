geom_boxplot(fill = "pink") +
labs(title = "Blood Glucose Level by Diabetes Status", x = "Diabetes", y = "Blood Glucose Level")
# Split the data into training and test sets
set.seed(42)
trainIndex <- createDataPartition(data$diabetes, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Train logistic regression model
model <- glm(diabetes ~ ., data = trainData, family = binomial)
# Predict on test data
predictions <- predict(model, testData, type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Evaluate the model
conf_matrix <- table(Predicted = predicted_classes, Actual = testData$diabetes)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")
# Detailed evaluation
library(caret)
confusionMatrix(as.factor(predicted_classes), as.factor(testData$diabetes))
# Load necessary libraries
library(tidyverse)
library(dplyr)
library(randomForest)
library(e1071)
library(pROC)
library(ggplot2)
# Load the dataset
data <- read.csv("D:\\r_diabetes\\diabetes\\diabetes_prediction_dataset.csv")
# Preprocess the data
data$bmi[is.na(data$bmi)] <- mean(data$bmi, na.rm = TRUE)
data$HbA1c_level[is.na(data$HbA1c_level)] <- mean(data$HbA1c_level, na.rm = TRUE)
data$blood_glucose_level[is.na(data$blood_glucose_level)] <- mean(data$blood_glucose_level, na.rm = TRUE)
data$smoking_h[is.na(data$smoking_h)] <- "No Info"
data$gender <- as.numeric(factor(data$gender, levels = c("Female", "Male")))
data$smoking_h <- as.factor(data$smoking_h)
num_cols <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
data[num_cols] <- scale(data[num_cols])
# Split data into training and testing sets
set.seed(123)
sample_index <- sample(1:nrow(data), size = 0.3 * nrow(data))  # Use 30% of data for faster processing
train <- data[sample_index, ]
test <- data[-sample_index, ]
# --- Logistic Regression Model ---
model_glm <- glm(diabetes ~ ., data = train, family = "binomial")
prob_glm <- predict(model_glm, test, type = "response")
roc_glm <- roc(test$diabetes, prob_glm)
auc_glm <- auc(roc_glm)
# --- Random Forest Model ---
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 50, mtry = 3)  # Reduced ntree for faster training
# Load necessary libraries
library(tidyverse)
library(dplyr)
library(randomForest)
library(e1071)
library(pROC)
library(ggplot2)
# Load the dataset
data <- read.csv("D:\\r_diabetes\\diabetes\\diabetes_prediction_dataset.csv")
# Preprocess the data
# Fill missing values in numeric columns with the column mean
data$bmi[is.na(data$bmi)] <- mean(data$bmi, na.rm = TRUE)
data$HbA1c_level[is.na(data$HbA1c_level)] <- mean(data$HbA1c_level, na.rm = TRUE)
data$blood_glucose_level[is.na(data$blood_glucose_level)] <- mean(data$blood_glucose_level, na.rm = TRUE)
# Fill missing values in categorical columns
data$smoking_h[is.na(data$smoking_h)] <- "No Info"
data$gender <- as.numeric(factor(data$gender, levels = c("Female", "Male")))
# Normalize numerical columns
num_cols <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
data[num_cols] <- scale(data[num_cols])
# Check for any remaining missing values and remove rows with NA, if any
data <- na.omit(data)
# Split data into training and testing sets
set.seed(123)
sample_index <- sample(1:nrow(data), size = 0.3 * nrow(data))  # Use 30% of data for faster processing
train <- data[sample_index, ]
test <- data[-sample_index, ]
# --- Logistic Regression Model ---
model_glm <- glm(diabetes ~ ., data = train, family = "binomial")
prob_glm <- predict(model_glm, test, type = "response")
roc_glm <- roc(test$diabetes, prob_glm)
auc_glm <- auc(roc_glm)
# --- Random Forest Model ---
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 50, mtry = 3)  # Reduced ntree for faster training
prob_rf <- predict(model_rf, test, type = "prob")[,2]
library(tidyverse)
library(dplyr)
# Load the dataset# Using double backslashes
data <- read.csv("D:\\r_diabetes\\diabetes\\diabetes_prediction_dataset.csv")
# View the structure of the dataset
str(data)
summary(data)
# Check for missing values
colSums(is.na(data))
# Fill missing values
data$bmi[is.na(data$bmi)] <- mean(data$bmi, na.rm = TRUE)
data$HbA1c_level[is.na(data$HbA1c_level)] <- mean(data$HbA1c_level, na.rm = TRUE)
data$blood_glucose_level[is.na(data$blood_glucose_level)] <- mean(data$blood_glucose_level, na.rm = TRUE)
data$smoking_h[is.na(data$smoking_h)] <- "No Info"
# Encode categorical variables
# Normalize numerical columns
data$gender <- as.numeric(factor(data$gender, levels = c("Female", "Male")))
data$smoking_h <- as.factor(data$smoking_h)
num_cols <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
data[num_cols] <- scale(data[num_cols])
# Load libraries for visualization
library(ggplot2)
# Count plot for diabetes
ggplot(data, aes(x = as.factor(diabetes))) +
geom_bar(fill = "skyblue") +
labs(title = "Diabetes Count", x = "Diabetes", y = "Count")
# Boxplot for age by diabetes status
ggplot(data, aes(x = as.factor(diabetes), y = age)) +
geom_boxplot(fill = "orange") +
labs(title = "Age by Diabetes Status", x = "Diabetes", y = "Age")
# Boxplot for BMI by diabetes status
ggplot(data, aes(x = as.factor(diabetes), y = bmi)) +
geom_boxplot(fill = "lightgreen") +
labs(title = "BMI by Diabetes Status", x = "Diabetes", y = "BMI")
# Boxplot for HbA1c level by diabetes status
ggplot(data, aes(x = as.factor(diabetes), y = HbA1c_level)) +
geom_boxplot(fill = "lightblue") +
labs(title = "HbA1c Level by Diabetes Status", x = "Diabetes", y = "HbA1c Level")
# Boxplot for blood glucose level by diabetes status
ggplot(data, aes(x = as.factor(diabetes), y = blood_glucose_level)) +
geom_boxplot(fill = "pink") +labs(title = "Blood Glucose Level by Diabetes Status", x = "Diabetes", y = "Blood Glucose Level")
# Set a random seed for reproducibility
set.seed(42)
# Define the proportion of the dataset to use for training
train_proportion <- 0.8
train_size <- floor(train_proportion * nrow(data))
train_indices <- sample(seq_len(nrow(data)), size = train_size)
trainData <- data[train_indices, ]
testData <- data[-train_indices, ]
cat("Training Set Size:", nrow(trainData), "\n")
cat("Test Set Size:", nrow(testData), "\n")
# Train the logistic regression model
model <- glm(diabetes ~ ., data = trainData, family = binomial)
# Predict on test data
predictions <- predict(model, testData, type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Calculate accuracy
accuracy <- mean(predictions == testData$diabetes)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Generate the confusion matrix manually
conf_matrix <- table(Predicted = predicted_classes, Actual = testData$diabetes)
print(conf_matrix)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
sensitivity <- conf_matrix[1,1] / sum(conf_matrix[,1]) # For class 0 as positive
specificity <- conf_matrix[2,2] / sum(conf_matrix[,2]) # For class 1 as positive
pos_pred_value <- conf_matrix[1,1] / sum(conf_matrix[1,])
neg_pred_value <- conf_matrix[2,2] / sum(conf_matrix[2,])
error <- qnorm(0.975) * sqrt((accuracy * (1 - accuracy)) / sum(conf_matrix))
lower_ci <- accuracy - error
upper_ci <- accuracy + error
cat("95% Confidence Interval for Accuracy:", lower_ci, "-", upper_ci, "\n")
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("Positive Predictive Value:", pos_pred_value, "\n")
cat("Negative Predictive Value:", neg_pred_value, "\n")
#2nd algorithm
install.packages("randomForest")
# Load the randomForest package
library(randomForest)
# Step 1: Ensure there are no missing values in the data
# Replace any empty strings with NA, then check for missing values
data[data == ""] <- NA
data <- na.omit(data)  # Remove rows with any NA values
# Step 2: Ensure categorical variables are factors
# Convert necessary columns to factors (adjust column names as per your dataset)
data$gender <- as.factor(data$gender)
data$hypertension <- as.factor(data$hypertension)
data$heart_disease <- as.factor(data$heart_disease)
data$smoking_history <- as.factor(data$smoking_history)
data$diabetes <- as.factor(data$diabetes)  # Make sure the target variable is also a factor
# Step 3: Split the data into training and testing sets
set.seed(123)
sample_index <- sample(1:nrow(data), size = 0.8 * nrow(data))
train <- data[sample_index, ]
test <- data[-sample_index, ]
# Step 4: Train the Random Forest model
# Set parameters: ntree (number of trees) and mtry (number of features to consider at each split)
model <- randomForest(diabetes ~ ., data = train, ntree = 100, mtry = 3, importance = TRUE)
# Load necessary libraries
library(tidyverse)
library(dplyr)
library(randomForest)
library(pROC)
library(ggplot2)
# Load the dataset
data <- read.csv("D:\\r_diabetes\\diabetes\\diabetes_prediction_dataset.csv")
# Preprocess the data
# Fill missing values in numeric columns with the column mean
data$bmi[is.na(data$bmi)] <- mean(data$bmi, na.rm = TRUE)
data$HbA1c_level[is.na(data$HbA1c_level)] <- mean(data$HbA1c_level, na.rm = TRUE)
data$blood_glucose_level[is.na(data$blood_glucose_level)] <- mean(data$blood_glucose_level, na.rm = TRUE)
# Fill missing values in categorical columns
data$smoking_h[is.na(data$smoking_h)] <- "No Info"
data$gender <- as.numeric(factor(data$gender, levels = c("Female", "Male")))
# Normalize numerical columns
num_cols <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
data[num_cols] <- scale(data[num_cols])
# Check for any remaining missing values and remove rows with NA, if any
data <- na.omit(data)
# Split data into training and testing sets
set.seed(123)
sample_index <- sample(1:nrow(data), size = 0.3 * nrow(data))  # Use 30% of data for faster processing
train <- data[sample_index, ]
test <- data[-sample_index, ]
# --- Logistic Regression Model ---
model_glm <- glm(diabetes ~ ., data = train, family = "binomial")
prob_glm <- predict(model_glm, test, type = "response")
roc_glm <- roc(test$diabetes, prob_glm)
auc_glm <- auc(roc_glm)
# --- Random Forest Model ---
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 50, mtry = 3)  # Reduced ntree for faster training
prob_rf <- predict(model_rf, test, type = "prob")[,2]
# Load necessary libraries
library(tidyverse)
library(dplyr)
library(randomForest)
library(pROC)
library(ggplot2)
# Load the dataset
data <- read.csv("D:\\r_diabetes\\diabetes\\diabetes_prediction_dataset.csv")
# Preprocess the data
# Fill missing values in numeric columns with the column mean
data$bmi[is.na(data$bmi)] <- mean(data$bmi, na.rm = TRUE)
data$HbA1c_level[is.na(data$HbA1c_level)] <- mean(data$HbA1c_level, na.rm = TRUE)
data$blood_glucose_level[is.na(data$blood_glucose_level)] <- mean(data$blood_glucose_level, na.rm = TRUE)
# Fill missing values in categorical columns
data$smoking_h[is.na(data$smoking_h)] <- "No Info"
data$gender <- as.numeric(factor(data$gender, levels = c("Female", "Male")))
# Ensure 'diabetes' is a factor for classification
data$diabetes <- as.factor(data$diabetes)
# Normalize numerical columns
num_cols <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
data[num_cols] <- scale(data[num_cols])
# Check for any remaining missing values and remove rows with NA, if any
data <- na.omit(data)
# Split data into training and testing sets
set.seed(123)
sample_index <- sample(1:nrow(data), size = 0.3 * nrow(data))  # Use 30% of data for faster processing
train <- data[sample_index, ]
test <- data[-sample_index, ]
# --- Logistic Regression Model ---
model_glm <- glm(diabetes ~ ., data = train, family = "binomial")
prob_glm <- predict(model_glm, test, type = "response")
roc_glm <- roc(test$diabetes, prob_glm)
auc_glm <- auc(roc_glm)
# --- Random Forest Model ---
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 50, mtry = 3)  # Reduced ntree for faster training
prob_rf <- predict(model_rf, test, type = "prob")[,2]
roc_rf <- roc(test$diabetes, prob_rf)
auc_rf <- auc(roc_rf)
# --- Plot ROC Curves ---
plot(roc_glm, col = "red", main = "AUC-ROC Curve Comparison for GLM and Random Forest", lwd = 2, ylim = c(0, 1))
lines(roc_rf, col = "blue", lwd = 2)
# Add legend with AUC values
legend("bottomright", legend = c(paste("GLM (AUC =", round(auc_glm, 2), ")"),
paste("Random Forest (AUC =", round(auc_rf, 2), ")")),
col = c("red", "blue"), lwd = 2)
# Print AUC values for each model
print(paste("GLM AUC:", round(auc_glm, 2)))
print(paste("Random Forest AUC:", round(auc_rf, 2)))
# Load necessary libraries
library(tidyverse)
library(dplyr)
library(randomForest)
library(pROC)
library(ggplot2)
# Load the dataset
data <- read.csv("D:\\r_diabetes\\diabetes\\diabetes_prediction_dataset.csv")
# Preprocess the data
data$bmi[is.na(data$bmi)] <- mean(data$bmi, na.rm = TRUE)
data$HbA1c_level[is.na(data$HbA1c_level)] <- mean(data$HbA1c_level, na.rm = TRUE)
data$blood_glucose_level[is.na(data$blood_glucose_level)] <- mean(data$blood_glucose_level, na.rm = TRUE)
data$smoking_h[is.na(data$smoking_h)] <- "No Info"
data$gender <- as.numeric(factor(data$gender, levels = c("Female", "Male")))
data$diabetes <- as.factor(data$diabetes)
num_cols <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
data[num_cols] <- scale(data[num_cols])
data <- na.omit(data)
# Split data
set.seed(123)
sample_index <- sample(1:nrow(data), size = 0.3 * nrow(data))
train <- data[sample_index, ]
test <- data[-sample_index, ]
# --- Logistic Regression Model ---
model_glm <- glm(diabetes ~ ., data = train, family = "binomial")
prob_glm <- predict(model_glm, test, type = "response")
roc_glm <- roc(test$diabetes, prob_glm)
auc_glm <- auc(roc_glm)
# --- Random Forest Model ---
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 50, mtry = 3)
prob_rf <- predict(model_rf, test, type = "prob")[,2]
roc_rf <- roc(test$diabetes, prob_rf)
auc_rf <- auc(roc_rf)
# --- Plots ---
# 1. Box Plot: Age by Diabetes Status
ggplot(data, aes(x = diabetes, y = age)) +
geom_boxplot(fill = "lightblue") +
labs(title = "Box Plot of Age by Diabetes Status", x = "Diabetes", y = "Age")
# 2. Bar Plot: Count of Diabetes Status
ggplot(data, aes(x = diabetes)) +
geom_bar(fill = "skyblue") +
labs(title = "Count of Diabetes Status", x = "Diabetes", y = "Count")
# 3. Density Plot: HbA1c Level by Diabetes Status
ggplot(data, aes(x = HbA1c_level, fill = diabetes)) +
geom_density(alpha = 0.5) +
labs(title = "Density Plot of HbA1c Level by Diabetes Status", x = "HbA1c Level", y = "Density")
# 4. Scatter Plot: BMI vs. Blood Glucose Level by Diabetes Status
ggplot(data, aes(x = bmi, y = blood_glucose_level, color = diabetes)) +
geom_point(alpha = 0.6) +
labs(title = "Scatter Plot of BMI vs. Blood Glucose Level", x = "BMI", y = "Blood Glucose Level")
# --- ROC Curve Plot ---
plot(roc_glm, col = "red", main = "AUC-ROC Curve for GLM and Random Forest", lwd = 2, ylim = c(0, 1))
lines(roc_rf, col = "blue", lwd = 2)
legend("bottomright", legend = c(paste("GLM (AUC =", round(auc_glm, 2), ")"),
paste("Random Forest (AUC =", round(auc_rf, 2), ")")),
col = c("red", "blue"), lwd = 2)
# Print AUC values
print(paste("GLM AUC:", round(auc_glm, 2)))
print(paste("Random Forest AUC:", round(auc_rf, 2)))
source("D:/r_diabetes/diabetes/new r project.R")
# Load necessary libraries
library(tidyverse)
library(dplyr)
library(randomForest)
library(pROC)
library(ggplot2)
# Load the dataset
data <- read.csv("D:\\r_diabetes\\diabetes\\diabetes_prediction_dataset.csv")
# Preprocess the data
data$bmi[is.na(data$bmi)] <- mean(data$bmi, na.rm = TRUE)
data$HbA1c_level[is.na(data$HbA1c_level)] <- mean(data$HbA1c_level, na.rm = TRUE)
data$blood_glucose_level[is.na(data$blood_glucose_level)] <- mean(data$blood_glucose_level, na.rm = TRUE)
data$smoking_h[is.na(data$smoking_h)] <- "No Info"
data$gender <- as.numeric(factor(data$gender, levels = c("Female", "Male")))
data$diabetes <- as.factor(data$diabetes)
num_cols <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
data[num_cols] <- scale(data[num_cols])
data <- na.omit(data)
# Split data
set.seed(123)
sample_index <- sample(1:nrow(data), size = 0.3 * nrow(data))
train <- data[sample_index, ]
test <- data[-sample_index, ]
# --- Logistic Regression Model ---
model_glm <- glm(diabetes ~ ., data = train, family = "binomial")
prob_glm <- predict(model_glm, test, type = "response")
roc_glm <- roc(test$diabetes, prob_glm)
auc_glm <- auc(roc_glm)
# --- Random Forest Model ---
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 50, mtry = 3)
prob_rf <- predict(model_rf, test, type = "prob")[,2]
roc_rf <- roc(test$diabetes, prob_rf)
auc_rf <- auc(roc_rf)
# --- Plots ---
# 1. Box Plot: Age by Diabetes Status
ggplot(data, aes(x = diabetes, y = age)) +
geom_boxplot(fill = "lightblue") +
labs(title = "Box Plot of Age by Diabetes Status", x = "Diabetes", y = "Age")
# 2. Bar Plot: Count of Diabetes Status
ggplot(data, aes(x = diabetes)) +
geom_bar(fill = "skyblue") +
labs(title = "Count of Diabetes Status", x = "Diabetes", y = "Count")
# 3. Density Plot: HbA1c Level by Diabetes Status
ggplot(data, aes(x = HbA1c_level, fill = diabetes)) +
geom_density(alpha = 0.5) +
labs(title = "Density Plot of HbA1c Level by Diabetes Status", x = "HbA1c Level", y = "Density")
# 4. Scatter Plot: BMI vs. Blood Glucose Level by Diabetes Status
ggplot(data, aes(x = bmi, y = blood_glucose_level, color = diabetes)) +
geom_point(alpha = 0.6) +
labs(title = "Scatter Plot of BMI vs. Blood Glucose Level", x = "BMI", y = "Blood Glucose Level")
# --- ROC Curve Plot ---
plot(roc_glm, col = "red", main = "AUC-ROC Curve for GLM and Random Forest", lwd = 2, ylim = c(0, 1))
lines(roc_rf, col = "blue", lwd = 2)
legend("bottomright", legend = c(paste("GLM (AUC =", round(auc_glm, 2), ")"),
paste("Random Forest (AUC =", round(auc_rf, 2), ")")),
col = c("red", "blue"), lwd = 2)
# Print AUC values
print(paste("GLM AUC:", round(auc_glm, 2)))
print(paste("Random Forest AUC:", round(auc_rf, 2)))
# For GLM model
pred_glm <- ifelse(prob_glm > 0.5, 1, 0)
conf_matrix_glm <- table(pred_glm, test$diabetes)
accuracy_glm <- sum(diag(conf_matrix_glm)) / sum(conf_matrix_glm)
precision_glm <- conf_matrix_glm[2,2] / sum(conf_matrix_glm[2,])
recall_glm <- conf_matrix_glm[2,2] / sum(conf_matrix_glm[2,1], conf_matrix_glm[2,2])
fscore_glm <- 2 * (precision_glm * recall_glm) / (precision_glm + recall_glm)
# For Random Forest model
pred_rf <- ifelse(prob_rf > 0.5, 1, 0)
conf_matrix_rf <- table(pred_rf, test$diabetes)
accuracy_rf <- sum(diag(conf_matrix_rf)) / sum(conf_matrix_rf)
precision_rf <- conf_matrix_rf[2,2] / sum(conf_matrix_rf[2,])
recall_rf <- conf_matrix_rf[2,2] / sum(conf_matrix_rf[2,1], conf_matrix_rf[2,2])
fscore_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
# Print metrics for both models
cat("GLM Accuracy:", accuracy_glm, "\n")
cat("GLM Precision:", precision_glm, "\n")
cat("GLM Recall:", recall_glm, "\n")
cat("GLM F-Score:", fscore_glm, "\n")
cat("Random Forest Accuracy:", accuracy_rf, "\n")
cat("Random Forest Precision:", precision_rf, "\n")
cat("Random Forest Recall:", recall_rf, "\n")
cat("Random Forest F-Score:", fscore_rf, "\n")
# Load necessary libraries
library(tidyverse)
library(dplyr)
library(randomForest)
library(pROC)
library(ggplot2)
# Load the dataset
data <- read.csv("D:\\r_diabetes\\diabetes\\diabetes_prediction_dataset.csv")
# Preprocess the data
data$bmi[is.na(data$bmi)] <- mean(data$bmi, na.rm = TRUE)
data$HbA1c_level[is.na(data$HbA1c_level)] <- mean(data$HbA1c_level, na.rm = TRUE)
data$blood_glucose_level[is.na(data$blood_glucose_level)] <- mean(data$blood_glucose_level, na.rm = TRUE)
data$smoking_h[is.na(data$smoking_h)] <- "No Info"
data$gender <- as.numeric(factor(data$gender, levels = c("Female", "Male")))
data$diabetes <- as.factor(data$diabetes)
num_cols <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
data[num_cols] <- scale(data[num_cols])
data <- na.omit(data)
# Split data into training and testing sets
set.seed(123)
sample_index <- sample(1:nrow(data), size = 0.3 * nrow(data))
train <- data[sample_index, ]
test <- data[-sample_index, ]
# --- Logistic Regression Model ---
model_glm <- glm(diabetes ~ ., data = train, family = "binomial")
prob_glm <- predict(model_glm, test, type = "response")
roc_glm <- roc(test$diabetes, prob_glm)
auc_glm <- auc(roc_glm)
# --- Random Forest Model ---
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 50, mtry = 3)
prob_rf <- predict(model_rf, test, type = "prob")[,2]
roc_rf <- roc(test$diabetes, prob_rf)
auc_rf <- auc(roc_rf)
# --- Calculate Metrics ---
# For GLM model
pred_glm <- ifelse(prob_glm > 0.5, 1, 0)
conf_matrix_glm <- table(pred_glm, test$diabetes)
accuracy_glm <- sum(diag(conf_matrix_glm)) / sum(conf_matrix_glm)
precision_glm <- conf_matrix_glm[2,2] / sum(conf_matrix_glm[2,])
recall_glm <- conf_matrix_glm[2,2] / sum(conf_matrix_glm[2,1], conf_matrix_glm[2,2])
fscore_glm <- 2 * (precision_glm * recall_glm) / (precision_glm + recall_glm)
# For Random Forest model
pred_rf <- ifelse(prob_rf > 0.5, 1, 0)
conf_matrix_rf <- table(pred_rf, test$diabetes)
accuracy_rf <- sum(diag(conf_matrix_rf)) / sum(conf_matrix_rf)
precision_rf <- conf_matrix_rf[2,2] / sum(conf_matrix_rf[2,])
recall_rf <- conf_matrix_rf[2,2] / sum(conf_matrix_rf[2,1], conf_matrix_rf[2,2])
fscore_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
# --- Print Metrics ---
cat("GLM Accuracy:", accuracy_glm, "\n")
cat("GLM Precision:", precision_glm, "\n")
cat("GLM Recall:", recall_glm, "\n")
cat("GLM F-Score:", fscore_glm, "\n")
cat("Random Forest Accuracy:", accuracy_rf, "\n")
cat("Random Forest Precision:", precision_rf, "\n")
cat("Random Forest Recall:", recall_rf, "\n")
cat("Random Forest F-Score:", fscore_rf, "\n")
# --- Create Comparison Plot ---
# Store metrics in a data frame
metrics <- data.frame(
Model = c("GLM", "Random Forest"),
Accuracy = c(accuracy_glm, accuracy_rf),
Precision = c(precision_glm, precision_rf),
Recall = c(recall_glm, recall_rf),
F_Score = c(fscore_glm, fscore_rf)
)
# Reshape the data to a long format for plotting
metrics_long <- metrics %>%
pivot_longer(cols = c(Accuracy, Precision, Recall, F_Score),
names_to = "Metric",
values_to = "Value")
# Plot comparison of accuracy, precision, recall, and F-score
ggplot(metrics_long, aes(x = Model, y = Value, fill = Metric)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Comparison of Accuracy, Precision, Recall, and F-Score",
x = "Model", y = "Value") +
scale_fill_manual(values = c("lightblue", "lightgreen", "lightcoral", "lightyellow")) +
theme_minimal()
install.packages("ggplot2")
library(ggplot2)
exam_scores <- c(45, 56, 67, 78, 89, 90, 93, 100, 110, 120, 55, 65, 75, 85, 95, 105, 115, 125, 135, 145)
# Create a Q-Q plot
qqnorm(exam_scores, main = "Q-Q Plot of Exam Scores")
qqline(exam_scores)
sales_data <- c(200, 220, 210, 230, 180, 190, 195, 205, 215, 225, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330)
hist(sales_data,
main = "Monthly Sales Histogram",
xlab = "Monthly Sales",
ylab = "Frequency",
col = "lightblue",
border = "black")
ratings <- c(4, 5, 3, 4, 5, 2, 4, 5, 3, 4, 5, 2, 4, 5, 3, 4, 5, 2, 4, 5)
barplot(table(ratings),
main = "Product Ratings",
xlab = "Rating",
ylab = "Frequency",
col = "lightblue")
height <- c(150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 152, 157, 162, 167, 172, 177, 182, 187, 192, 197)
weight <- c(50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 52, 57, 62, 67, 72, 77, 82, 87, 92, 97)
plot(height, weight,
xlab = "Height (cm)",
ylab = "Weight (kg)",
main = "Height vs. Weight Scatter Plot",
pch = 19,
col = "blue")
